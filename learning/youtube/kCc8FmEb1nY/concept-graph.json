{
  "metadata": {
    "title": "Let's build GPT: from scratch, in code, spelled out",
    "author": "Andrej Karpathy",
    "source": "https://www.youtube.com/watch?v=kCc8FmEb1nY",
    "video_id": "kCc8FmEb1nY",
    "total_duration": 6979.46,
    "total_concepts": 30,
    "extracted_at": "2025-12-12T11:21:31.442Z"
  },
  "nodes": [
    {
      "id": "language_model",
      "name": "Language Model",
      "description": "A probabilistic system that models the sequence of words, characters, or tokens, predicting what comes next in a sequence given previous context.",
      "prerequisites": [],
      "difficulty": "basic"
    },
    {
      "id": "character_level_language_model",
      "name": "Character-Level Language Model",
      "description": "A specific type of language model that operates on individual characters as its fundamental units, predicting the next character in a sequence.",
      "prerequisites": [
        "language_model"
      ],
      "difficulty": "basic"
    },
    {
      "id": "tokenization",
      "name": "Tokenization",
      "description": "The process of converting raw text (string) into a sequence of numerical identifiers (integers) based on a defined vocabulary. This can be at character, sub-word, or word level.",
      "prerequisites": [
        "language_model"
      ],
      "difficulty": "basic"
    },
    {
      "id": "vocabulary",
      "name": "Vocabulary",
      "description": "The complete set of unique characters, sub-words, or words that a language model can recognize and generate, each mapped to a unique integer ID.",
      "prerequisites": [
        "tokenization"
      ],
      "difficulty": "basic"
    },
    {
      "id": "data_splits",
      "name": "Data Splits (Train/Validation)",
      "description": "The practice of dividing a dataset into distinct subsets: a training set for model learning and a validation set for evaluating generalization and detecting overfitting.",
      "prerequisites": [],
      "difficulty": "basic"
    },
    {
      "id": "context_window",
      "name": "Context Window (Block Size)",
      "description": "The maximum fixed number of previous tokens (or characters) that a language model considers as context when predicting the next token in a sequence.",
      "prerequisites": [
        "language_model"
      ],
      "difficulty": "basic"
    },
    {
      "id": "batch_processing",
      "name": "Batch Processing",
      "description": "The technique of grouping multiple independent input sequences (or data samples) into a single tensor (a \"batch\") to enable efficient parallel computation on hardware like GPUs.",
      "prerequisites": [],
      "difficulty": "basic"
    },
    {
      "id": "input_target_pairing",
      "name": "Input-Target Pairing for Language Modeling",
      "description": "The method of creating training examples from a sequence where the input (X) is a segment of the sequence and the target (Y) is the same segment shifted one position to the right, used to predict the next element at each position.",
      "prerequisites": [
        "language_model",
        "context_window"
      ],
      "difficulty": "basic"
    },
    {
      "id": "token_embeddings",
      "name": "Token Embeddings",
      "description": "Dense, low-dimensional vector representations for discrete tokens (characters, words) that capture their semantic meaning, learned during training.",
      "prerequisites": [
        "tokenization",
        "vocabulary"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "positional_embeddings",
      "name": "Positional Embeddings",
      "description": "Vectors added to token embeddings to inject information about the absolute or relative position of each token within a sequence, as attention mechanisms are inherently permutation-invariant.",
      "prerequisites": [
        "token_embeddings",
        "context_window"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "logits_and_softmax",
      "name": "Logits and Softmax",
      "description": "Logits are the raw, unnormalized scores output by a neural network for each possible next token. Softmax is an activation function that converts these logits into a probability distribution over the vocabulary.",
      "prerequisites": [
        "token_embeddings"
      ],
      "difficulty": "basic"
    },
    {
      "id": "cross_entropy_loss",
      "name": "Cross-Entropy Loss",
      "description": "A commonly used loss function in classification and language modeling tasks that measures the difference between the predicted probability distribution (from softmax) and the true probability distribution (one-hot encoded target).",
      "prerequisites": [
        "logits_and_softmax",
        "input_target_pairing"
      ],
      "difficulty": "basic"
    },
    {
      "id": "model_generation_sampling",
      "name": "Model Generation (Sampling)",
      "description": "The process of iteratively predicting and appending new tokens to a sequence using a trained language model's probability distribution (e.g., via multinomial sampling) to produce novel text.",
      "prerequisites": [
        "language_model",
        "logits_and_softmax"
      ],
      "difficulty": "basic"
    },
    {
      "id": "gradient_descent_optimization",
      "name": "Gradient Descent Optimization",
      "description": "A general class of iterative algorithms used to minimize a loss function by adjusting model parameters in the direction opposite to the gradient of the loss with respect to those parameters.",
      "prerequisites": [
        "cross_entropy_loss"
      ],
      "difficulty": "basic"
    },
    {
      "id": "weighted_aggregation",
      "name": "Weighted Aggregation",
      "description": "A mathematical technique where a summary vector is formed by taking a weighted sum of multiple input vectors, with weights indicating the importance or relevance of each input. This is a foundational concept for attention.",
      "prerequisites": [],
      "difficulty": "basic"
    },
    {
      "id": "causal_masking",
      "name": "Causal Masking (Lower Triangular Matrix)",
      "description": "A technique used in auto-regressive models like decoder-only Transformers where a lower triangular matrix is applied to attention scores to prevent tokens from attending to (gaining information from) future tokens in the sequence.",
      "prerequisites": [
        "weighted_aggregation"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "query_key_value_vectors",
      "name": "Query, Key, and Value Vectors",
      "description": "In attention mechanisms, each token generates three vectors: a Query (what am I looking for?), a Key (what information do I contain?), and a Value (what information would I communicate if found interesting?).",
      "prerequisites": [
        "token_embeddings"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "scaled_dot_product_attention",
      "name": "Scaled Dot-Product Attention",
      "description": "The fundamental attention mechanism that calculates attention scores by taking the dot product of Queries with Keys, scaling them by the square root of the head size, applying a softmax to get weights, and then performing a weighted aggregation of Values.",
      "prerequisites": [
        "query_key_value_vectors",
        "weighted_aggregation",
        "causal_masking",
        "logits_and_softmax"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "self_attention",
      "name": "Self-Attention",
      "description": "An attention mechanism where the Query, Key, and Value vectors for all tokens originate from the same input sequence, allowing tokens within that sequence to attend to each other.",
      "prerequisites": [
        "scaled_dot_product_attention"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "multi_head_attention",
      "name": "Multi-Head Attention",
      "description": "An extension of self-attention that runs multiple self-attention mechanisms (heads) in parallel, each learning different relationships. Their outputs are then concatenated and linearly transformed.",
      "prerequisites": [
        "self_attention"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "feed_forward_network_transformer",
      "name": "Feed-Forward Network (Transformer Component)",
      "description": "A simple multi-layer perceptron applied independently to each token's representation after the attention mechanism within a Transformer block, allowing the model to perform per-token computation.",
      "prerequisites": [
        "multi_head_attention"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "residual_connections",
      "name": "Residual Connections (Skip Connections)",
      "description": "Architectural elements that add the input of a layer (or block) directly to its output, creating a \"skip\" pathway for gradients and enabling the training of very deep neural networks.",
      "prerequisites": [
        "gradient_descent_optimization"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "layer_normalization",
      "name": "Layer Normalization",
      "description": "A normalization technique applied across the features of each individual sample (rather than across the batch) to stabilize activations and improve training dynamics in deep neural networks, particularly in Transformers.",
      "prerequisites": [
        "residual_connections"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "transformer_block",
      "name": "Transformer Block",
      "description": "The fundamental repeating unit of the Transformer architecture, combining multi-head attention, a feed-forward network, residual connections, and layer normalization to process sequences effectively.",
      "prerequisites": [
        "multi_head_attention",
        "feed_forward_network_transformer",
        "residual_connections",
        "layer_normalization"
      ],
      "difficulty": "advanced"
    },
    {
      "id": "decoder_only_transformer",
      "name": "Decoder-Only Transformer",
      "description": "A Transformer architecture (like GPT) consisting solely of decoder blocks (with causal masking), designed for auto-regressive language modeling and unconditional text generation.",
      "prerequisites": [
        "transformer_block",
        "causal_masking"
      ],
      "difficulty": "advanced"
    },
    {
      "id": "encoder_decoder_transformer",
      "name": "Encoder-Decoder Transformer",
      "description": "A Transformer architecture used for sequence-to-sequence tasks (e.g., machine translation), comprising an encoder (which processes the input sequence) and a decoder (which generates the output sequence, conditioned on the encoder's output).",
      "prerequisites": [
        "transformer_block"
      ],
      "difficulty": "advanced"
    },
    {
      "id": "cross_attention",
      "name": "Cross-Attention",
      "description": "An attention mechanism where the Query vectors come from one sequence (e.g., the decoder's current state) and the Key and Value vectors come from a different, external sequence (e.g., the encoder's output).",
      "prerequisites": [
        "scaled_dot_product_attention",
        "encoder_decoder_transformer"
      ],
      "difficulty": "advanced"
    },
    {
      "id": "pre_training_llm",
      "name": "Pre-training Large Language Models",
      "description": "The initial, unsupervised training stage for large language models on massive text corpora, where the model learns to predict the next token, resulting in a general-purpose text generator.",
      "prerequisites": [
        "decoder_only_transformer"
      ],
      "difficulty": "advanced"
    },
    {
      "id": "fine_tuning_llm",
      "name": "Fine-tuning Large Language Models",
      "description": "The subsequent supervised training stage where a pre-trained language model is adapted to specific downstream tasks (e.g., question answering, summarization) using smaller, task-specific datasets.",
      "prerequisites": [
        "pre_training_llm"
      ],
      "difficulty": "advanced"
    },
    {
      "id": "reinforcement_learning_human_feedback",
      "name": "Reinforcement Learning from Human Feedback (RLHF)",
      "description": "An advanced fine-tuning technique used to align large language models with human preferences, involving training a reward model from human rankings and then optimizing the language model using reinforcement learning (e.g., PPO) to maximize the predicted reward.",
      "prerequisites": [
        "fine_tuning_llm"
      ],
      "difficulty": "advanced"
    }
  ]
}